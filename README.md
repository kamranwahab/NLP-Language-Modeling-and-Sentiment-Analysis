# ğŸ§  NLP Language Modeling & Sentiment Analysis

### *Statistical NLP Fundamentals using NLTK*

---

<p align="center">
  <img src="https://img.shields.io/badge/NLP-N--gram%20Language%20Model-blue" />
  <img src="https://img.shields.io/badge/Model-Naive%20Bayes-orange" />
  <img src="https://img.shields.io/badge/Dataset-Reuters%20%7C%20Movie%20Reviews-green" />
  <img src="https://img.shields.io/badge/Status-Completed-success" />
  <img src="https://img.shields.io/badge/Level-Academic%20Assignment-lightgrey" />
</p>



---

## ğŸ§  Introduction / Summary

Natural Language Processing (NLP) enables machines to analyze and understand human language.

This assignment explores **two essential NLP questions**:

* **How likely is a sentence?**
  â†’ Learned using a **Bigram Language Model**

* **What sentiment does a sentence express?**
  â†’ Learned using **NaÃ¯ve Bayes sentiment classification**

### ğŸ”‘ Key Idea

> Simple statistical models, when implemented correctly, can perform **surprisingly well** without complex linguistic rules.


---

## ğŸ¯ Objectives

The main objectives of this assignment are to:

* âœ… Understand **N-gram language modeling**
* âœ… Learn **Add-1 (Laplace) smoothing**
* âœ… Compute **sentence probabilities**
* âœ… Measure **perplexity on unseen text**
* âœ… Implement **NaÃ¯ve Bayes classifier from scratch**
* âœ… Perform **binary sentiment analysis**
* âœ… Experiment with different linguistic features
* âœ… Compare **statistical vs rule-based approaches**

---


## ğŸ“‚ Datasets Used


### 1ï¸âƒ£ Reuters Corpus (NLTK)

**Category:** `acq` (Acquisitions)

**Purpose:** Language modeling (syntax)

**Used for:**

* Tokenization
* Unigram & Bigram construction
* Bigram probability estimation
* Perplexity calculation on unseen sentences

---

### 2ï¸âƒ£ Movie Reviews Corpus (NLTK)

**Type:** Binary sentiment dataset (Positive / Negative)

**Purpose:** Sentiment analysis (semantics)

**Used for:**

* Training NaÃ¯ve Bayes classifier
* Evaluating sentiment classification accuracy

---


## âš™ï¸ Tools & Technologies

* **Python**
* **NLTK (Natural Language Toolkit)**
* **NumPy**
* **Scikit-learn** *(only for train/test split)*
* **Google Colab**
* **Jupyter Notebook (.ipynb)**

---


### ğŸ”¹ Language Modeling Experiments

* Unigram and Bigram construction
* Add-1 (Laplace) smoothing
* Sentence probability comparison
* Perplexity evaluation on unseen text

---

### ğŸ”¹ Sentiment Classification Experiments

* Baseline NaÃ¯ve Bayes (Unigram features)
* Stopword removal
* Negation handling using `NOT_` prefix
* Lexicon-based features (`LEX_POS`, `LEX_NEG`)
* Bigram-based NaÃ¯ve Bayes classifier

---

---

## ğŸ“Š Results Summary

| Model / Experiment                 | Accuracy  |
| ---------------------------------- | --------- |
| **Baseline NaÃ¯ve Bayes (Unigram)** | **87.0%** |
| Stopword Removal                   | 85.5%     |
| Negation Handling                  | 85.5%     |
| Lexicon Features                   | 87.0%     |
| Bigram Features                    | 86.5%     |

**Language Model Perplexity:**
â¡ï¸ `982.98` on unseen sentences

---

---

## ğŸ§  Main Findings

* âœ” Bigram Language Models effectively learn **word order and grammar (syntax)**
* âœ” NaÃ¯ve Bayes effectively learns **sentiment and meaning (semantics)**
* âœ” Pure **statistical models outperformed rule-based tweaks**
* âœ” Stopwords and simple negation rules sometimes remove useful information
* âœ” Lexicon features were **redundant** for this dataset


---

## ğŸ§¾ How to Run the Project

1. Open **Google Colab**
2. Upload `NLP_Assignment1_Colab.ipynb`
3. Run all cells sequentially
4. Ensure **internet access** for NLTK dataset downloads

---


## â­ Why This Repository Matters

* ğŸ“Œ Demonstrates **strong NLP fundamentals**
* ğŸ“Œ Shows ability to **implement models from scratch**
* ğŸ“Œ Includes **experiments, analysis, and reflection**
