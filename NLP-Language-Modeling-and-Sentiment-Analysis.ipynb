{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WEWuInrBhLq3"
      },
      "outputs": [],
      "source": [
        "# Install requirements (if not already installed)\n",
        "# bash\n",
        "# pip install nitk pandas numpy scikit-learn matplotlib\n",
        "\n",
        "# Imports\n",
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "from nltk.corpus import reuters, movie_reviews, stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter, defaultdict\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from typing import List, Tuple, Dict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK corpora (if not already downloaded)\n",
        "# python\n",
        "nltk.download('reuters')\n",
        "nltk.download('movie_reviews')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HamLdaf6hkD9",
        "outputId": "a1499c7a-c47a-4fd5-f7ac-2dbc5784a22e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load and tokenize text\n",
        "docs = reuters.sents(categories='acq')[:2000]\n",
        "\n",
        "# Flatten the list of sentences into a single list of tokens\n",
        "tokens = []\n",
        "for sent in docs:\n",
        "    # Add start and end tokens, and convert words to lowercase\n",
        "    tokens.append('<s>')\n",
        "    tokens.extend([w.lower() for w in sent])\n",
        "    tokens.append('</s>')\n",
        "\n",
        "# Calculate the size of the vocabulary (V) for smoothing\n",
        "V = len(set(tokens))\n",
        "print(f\"Total tokens in corpus: {len(tokens)}\")\n",
        "print(f\"Vocabulary Size (V): {V}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62w-cbyfhr71",
        "outputId": "9a8339e9-4145-45c4-c6ac-ddd3349e1f85"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total tokens in corpus: 61659\n",
            "Vocabulary Size (V): 5576\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Build n-grams and counts\n",
        "def build_ngram_counts(token_list: List[str], n: int) -> Counter:\n",
        "    \"\"\"Computes and returns the counts of all n-grams in the token list.\"\"\"\n",
        "    return Counter(ngrams(token_list, n))\n",
        "\n",
        "# Calculate counts\n",
        "unigram_counts = build_ngram_counts(tokens, 1)\n",
        "bigram_counts = build_ngram_counts(tokens, 2)\n",
        "trigram_counts = build_ngram_counts(tokens, 3) # Optional, but good for context\n",
        "print(f\"Number of unique unigrams: {len(unigram_counts)}\")\n",
        "print(f\"Number of unique bigrams: {len(bigram_counts)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "viydPyOXhs9N",
        "outputId": "ac9eb4a3-fd17-42ec-c866-4d02b2941fb0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique unigrams: 5576\n",
            "Number of unique bigrams: 27351\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Define bigram probability function\n",
        "def bigram_prob(w1: str, w2: str) -> float:\n",
        "    \"\"\"\n",
        "    Calculates P(w2 | w1) using Add-1 smoothing (Laplace smoothing).\n",
        "    w1 is the previous word (context), w2 is the current word.\n",
        "    \"\"\"\n",
        "    # The counts in unigram_counts are tuples, e.g., ('the',)\n",
        "    w1_count = unigram_counts.get((w1,), 0)\n",
        "\n",
        "    # Use bigram_counts directly for the (w1, w2) pair\n",
        "    w1_w2_count = bigram_counts.get((w1, w2), 0)\n",
        "\n",
        "    # Apply Add-1 smoothing formula\n",
        "    return (w1_w2_count + 1) / (w1_count + V)\n",
        "\n",
        "# Test the function\n",
        "print(f\"P('the' | 'in'): {bigram_prob('in', 'the'):.6f}\")\n",
        "print(f\"P('unseen_word' | 'in'): {bigram_prob('in', 'unseen_word'):.6f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZr9Wg-ghxe1",
        "outputId": "200dfb01-14d8-4f9c-d145-b13f5a4f42ad"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P('the' | 'in'): 0.028483\n",
            "P('unseen_word' | 'in'): 0.000155\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Define sentence probability and perplexity functions\n",
        "\n",
        "def sentence_prob(sentence: str) -> float:\n",
        "    \"\"\"Computes the probability of a sentence based on the bigram model.\"\"\"\n",
        "    sent = ['<s>'] + word_tokenize(sentence.lower()) + ['</s>']\n",
        "    prob = 1.0\n",
        "\n",
        "    for i in range(1, len(sent)):\n",
        "        w1 = sent[i-1]\n",
        "        w2 = sent[i]\n",
        "        # We assume words not in the vocab are OOV and will be handled by smoothing.\n",
        "        prob *= bigram_prob(w1, w2)\n",
        "    return prob\n",
        "\n",
        "def perplexity(test_sents: List[List[str]]) -> float:\n",
        "    \"\"\"Computes the perplexity of the model on a set of test sentences.\"\"\"\n",
        "    N = 0  # Total number of tokens in the test set\n",
        "    log_prob_sum = 0.0\n",
        "\n",
        "    for s in test_sents:\n",
        "        # Add start/end tokens to the test sentence\n",
        "        sent = ['<s>'] + s + ['</s>']\n",
        "\n",
        "        # N is the number of word choices made, which is the length of the sentence *minus* 1\n",
        "        # In this formulation, it is simply the number of bigrams/tokens in the sentence\n",
        "        N += (len(sent) - 1)\n",
        "\n",
        "        for i in range(1, len(sent)):\n",
        "            w1 = sent[i-1]\n",
        "            w2 = sent[i]\n",
        "            p = bigram_prob(w1, w2)\n",
        "            # Use math.log for natural log, as standard in NLP for log probabilities\n",
        "            log_prob_sum += math.log(p)\n",
        "\n",
        "    # Perplexity formula: exp(-(1/N) * sum(log(P(w_i | w_{i-1}))))\n",
        "    return math.exp(-log_prob_sum / N)"
      ],
      "metadata": {
        "id": "zWfiyYqnh2j7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 1: Compute sentence probabilities\n",
        "\n",
        "sentence_a = \"the company made a profit\"\n",
        "sentence_b = \"profit company the made\"\n",
        "\n",
        "prob_a = sentence_prob(sentence_a)\n",
        "prob_b = sentence_prob(sentence_b)\n",
        "\n",
        "print(f\"Probability of '{sentence_a}': {prob_a}\")\n",
        "print(f\"Probability of '{sentence_b}': {prob_b}\")\n",
        "\n",
        "if prob_a > prob_b:\n",
        "    comparison = f\"'{sentence_a}' is more likely (higher probability).\"\n",
        "else:\n",
        "    comparison = f\"'{sentence_b}' is more likely (higher probability).\"\n",
        "\n",
        "print(f\"\\nComparison: {comparison}\")\n",
        "\n",
        "# Prepare 10 unseen sentences for Perplexity calculation\n",
        "# Use a small part of the reuters corpus not used for training\n",
        "unseen_docs = reuters.sents(categories='acq')[2000:2010]\n",
        "test_sents = [[w.lower() for w in s] for s in unseen_docs]\n",
        "\n",
        "ppl = perplexity(test_sents)\n",
        "print(f\"\\nPerplexity on 10 unseen sentences: {ppl:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2gNJxWyh6Ut",
        "outputId": "f33faa56-6eee-4c7c-a054-95f51f3525fe"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probability of 'the company made a profit': 2.607183099484686e-17\n",
            "Probability of 'profit company the made': 5.337703423711441e-19\n",
            "\n",
            "Comparison: 'the company made a profit' is more likely (higher probability).\n",
            "\n",
            "Perplexity on 10 unseen sentences: 982.98\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load sentiment dataset\n",
        "docs_raw = [(list(movie_reviews.words(fileid)), category)\n",
        "            for category in movie_reviews.categories()\n",
        "            for fileid in movie_reviews.fileids(category)]\n",
        "\n",
        "# Shuffle for good measure\n",
        "random.seed(42)\n",
        "random.shuffle(docs_raw)\n",
        "\n",
        "# Split into train and test sets\n",
        "train_raw, test_raw = train_test_split(docs_raw, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Training set size: {len(train_raw)}\")\n",
        "print(f\"Test set size: {len(test_raw)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sPB34WkiAiU",
        "outputId": "ebf6e048-a429-41c6-e576-63f508e859f6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 1600\n",
            "Test set size: 400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper to get the vocabulary and total counts\n",
        "def get_model_params(train_data: List[Tuple[List[str], str]]) -> Tuple[Dict[str, Counter], Dict[str, int], int]:\n",
        "    \"\"\"Builds word counts per class and computes total counts and vocabulary size.\"\"\"\n",
        "    word_counts = {'pos': Counter(), 'neg': Counter()}\n",
        "\n",
        "    # 1. Build word counts per class\n",
        "    for words, label in train_data:\n",
        "        # Convert words to lowercase before counting\n",
        "        word_counts[label].update(w.lower() for w in words)\n",
        "\n",
        "    # 2. Compute the vocabulary V\n",
        "    pos_words = set(word_counts['pos'].keys())\n",
        "    neg_words = set(word_counts['neg'].keys())\n",
        "    V = len(pos_words.union(neg_words))\n",
        "\n",
        "    # 3. Compute total token count per class\n",
        "    total_counts = {c: sum(word_counts[c].values()) for c in ['pos', 'neg']}\n",
        "\n",
        "    return word_counts, total_counts, V\n",
        "\n",
        "word_counts, total_counts, V = get_model_params(train_raw)"
      ],
      "metadata": {
        "id": "M9BzEgMfiBbI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Define probability functions\n",
        "\n",
        "def P_word_given_class(word: str, c: str, wc: Dict[str, Counter], tc: Dict[str, int], vocab_size: int) -> float:\n",
        "    \"\"\"Calculates P(word | class) using Add-1 smoothing.\"\"\"\n",
        "    # word_counts[c][word] + 1 / (total_counts[c] + V)\n",
        "    return (wc[c].get(word, 0) + 1) / (tc[c] + vocab_size)\n",
        "\n",
        "def P_class_given_doc(words: List[str], wc: Dict[str, Counter], tc: Dict[str, int], vocab_size: int) -> str:\n",
        "    \"\"\"Classifies a document (list of words) by finding the class with the max log probability.\"\"\"\n",
        "    probs = {}\n",
        "\n",
        "    for c in ['pos', 'neg']:\n",
        "        # Assume equal priors P(pos) = P(neg) = 0.5\n",
        "        log_prob = math.log(0.5)\n",
        "\n",
        "        for w in words:\n",
        "            # Add log probability of each word given the class\n",
        "            log_prob += math.log(P_word_given_class(w, c, wc, tc, vocab_size))\n",
        "\n",
        "        probs[c] = log_prob\n",
        "\n",
        "    # Return the class with the highest log probability\n",
        "    return max(probs, key=probs.get)"
      ],
      "metadata": {
        "id": "uwLBx_cTiENx"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Evaluate the baseline model\n",
        "\n",
        "def evaluate(test_data: List[Tuple[List[str], str]], wc: Dict[str, Counter], tc: Dict[str, int], vocab_size: int) -> float:\n",
        "    \"\"\"Evaluates the classifier on the test data.\"\"\"\n",
        "    correct = 0\n",
        "    # Use only the first 200 reviews as per assignment instructions\n",
        "    subset_test = test_data[:200]\n",
        "\n",
        "    for words, label in subset_test:\n",
        "        # Lowercase the words in the test document\n",
        "        lower_words = [w.lower() for w in words]\n",
        "\n",
        "        pred = P_class_given_doc(lower_words, wc, tc, vocab_size)\n",
        "        if pred == label:\n",
        "            correct += 1\n",
        "\n",
        "    accuracy = correct / len(subset_test)\n",
        "    return accuracy\n",
        "\n",
        "baseline_accuracy = evaluate(test_raw, word_counts, total_counts, V)\n",
        "print(f\"Baseline Accuracy (first 200 reviews): {baseline_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lq9GZociHqk",
        "outputId": "841e867d-8783-403b-dd2c-9d53f4127b49"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline Accuracy (first 200 reviews): 0.8700\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords_from_data(data: List[Tuple[List[str], str]]) -> List[Tuple[List[str], str]]:\n",
        "    \"\"\"Removes standard English stopwords from the words in each document.\"\"\"\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    new_data = []\n",
        "    for words, label in data:\n",
        "        # Keep only words not in the stop_words set\n",
        "        filtered_words = [w.lower() for w in words if w.lower() not in stop_words]\n",
        "        new_data.append((filtered_words, label))\n",
        "    return new_data\n",
        "\n",
        "# Apply stopword removal to the training data\n",
        "train_stop_removed = remove_stopwords_from_data(train_raw)\n",
        "\n",
        "# Re-train the model\n",
        "wc_stop, tc_stop, V_stop = get_model_params(train_stop_removed)\n",
        "\n",
        "# Evaluate the model (must also apply stopword removal to test data for consistency)\n",
        "test_stop_removed = remove_stopwords_from_data(test_raw)\n",
        "accuracy_stop = evaluate(test_stop_removed, wc_stop, tc_stop, V_stop)\n",
        "\n",
        "print(f\"Accuracy with Stopword Removal: {accuracy_stop:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sT3R_JFxiOOl",
        "outputId": "a2e0ca7c-37b5-4c84-f6c5-0df6fa14da03"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Stopword Removal: 0.8550\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def handle_negation(words: List[str]) -> List[str]:\n",
        "    \"\"\"Prepends 'NOT_' to words following negation terms until a punctuation mark is reached.\"\"\"\n",
        "    negation_terms = {\"not\", \"n't\", \"never\", \"no\", \"hardly\", \"barely\"}\n",
        "    punctuations = {'.', ',', ';', ':', '!', '?' , ')', '('} # Use a set of common delimiters\n",
        "    new_words = []\n",
        "    negate = False\n",
        "\n",
        "    for w in words:\n",
        "        w_lower = w.lower()\n",
        "\n",
        "        if w_lower in negation_terms or w_lower.endswith(\"n't\"):\n",
        "            negate = True\n",
        "            new_words.append(w_lower) # Keep the negation word itself\n",
        "            continue\n",
        "\n",
        "        if w_lower in punctuations:\n",
        "            negate = False\n",
        "            new_words.append(w_lower)\n",
        "            continue\n",
        "\n",
        "        if negate:\n",
        "            new_words.append(\"NOT_\" + w_lower)\n",
        "        else:\n",
        "            new_words.append(w_lower)\n",
        "\n",
        "    return new_words\n",
        "\n",
        "def apply_negation_to_data(data: List[Tuple[List[str], str]]) -> List[Tuple[List[str], str]]:\n",
        "    \"\"\"Applies negation handling to all documents in the dataset.\"\"\"\n",
        "    return [(handle_negation(words), label) for words, label in data]\n",
        "\n",
        "# Apply negation handling to the training data\n",
        "train_negation = apply_negation_to_data(train_raw)\n",
        "\n",
        "# Re-train the model\n",
        "wc_neg, tc_neg, V_neg = get_model_params(train_negation)\n",
        "\n",
        "# Evaluate the model\n",
        "test_negation = apply_negation_to_data(test_raw)\n",
        "accuracy_neg = evaluate(test_negation, wc_neg, tc_neg, V_neg)\n",
        "\n",
        "print(f\"Accuracy with Negation Handling: {accuracy_neg:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXPQcNLDiPCK",
        "outputId": "ec3b657a-8107-4010-f2d2-f9b2809fede7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Negation Handling: 0.8550\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lexicons provided in the assignment\n",
        "positive_lexicon = {\"good\", \"excellent\", \"great\", \"amazing\", \"love\", \"nice\"}\n",
        "negative_lexicon = {\"bad\", \"awful\", \"terrible\", \"hate\", \"boring\", \"poor\"}\n",
        "\n",
        "def add_lexicon_tokens(words: List[str]) -> List[str]:\n",
        "    \"\"\"Appends special tokens if positive/negative lexicon words are found.\"\"\"\n",
        "    new_words = list(words) # Start with a copy of the original (lowercased) words\n",
        "\n",
        "    # Check for presence of lexicon words\n",
        "    words_set = set(new_words)\n",
        "\n",
        "    if any(w in positive_lexicon for w in words_set):\n",
        "        new_words.append(\"LEX_POS\")\n",
        "\n",
        "    if any(w in negative_lexicon for w in words_set):\n",
        "        new_words.append(\"LEX_NEG\")\n",
        "\n",
        "    return new_words\n",
        "\n",
        "def apply_lexicon_to_data(data: List[Tuple[List[str], str]]) -> List[Tuple[List[str], str]]:\n",
        "    \"\"\"Applies lexicon token augmentation to all documents.\"\"\"\n",
        "    # Ensure words are lowercased before passing to add_lexicon_tokens\n",
        "    processed_data = []\n",
        "    for words, label in data:\n",
        "        lower_words = [w.lower() for w in words]\n",
        "        processed_data.append((add_lexicon_tokens(lower_words), label))\n",
        "    return processed_data\n",
        "\n",
        "# Apply lexicon feature addition to the training data\n",
        "train_lexicon = apply_lexicon_to_data(train_raw)\n",
        "\n",
        "# Re-train the model\n",
        "wc_lex, tc_lex, V_lex = get_model_params(train_lexicon)\n",
        "\n",
        "# Evaluate the model\n",
        "test_lexicon = apply_lexicon_to_data(test_raw)\n",
        "accuracy_lex = evaluate(test_lexicon, wc_lex, tc_lex, V_lex)\n",
        "\n",
        "print(f\"Accuracy with Lexicon Features: {accuracy_lex:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqiQZfiiiS9y",
        "outputId": "ee7b058f-3045-4e9a-fd04-50a49490e5c5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Lexicon Features: 0.8700\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Exercise 3 (Part 2): Naive Bayes with Bigram Features ---\n",
        "# We need to modify our functions to extract bigrams instead of unigrams.\n",
        "\n",
        "def get_model_params_bigrams(train_data: List[Tuple[List[str], str]]) -> Tuple[Dict[str, Counter], Dict[str, int], int]:\n",
        "    \"\"\"Builds bigram counts per class and computes total counts and vocabulary size.\"\"\"\n",
        "    word_counts = {'pos': Counter(), 'neg': Counter()}\n",
        "\n",
        "    # 1. Build bigram counts per class\n",
        "    for words, label in train_data:\n",
        "        # Convert words to lowercase\n",
        "        lower_words = [w.lower() for w in words]\n",
        "        # Create bigrams from the list of words\n",
        "        doc_bigrams = list(ngrams(lower_words, 2))\n",
        "        word_counts[label].update(doc_bigrams)\n",
        "\n",
        "    # 2. Compute the vocabulary V (set of unique bigrams)\n",
        "    pos_bigrams = set(word_counts['pos'].keys())\n",
        "    neg_bigrams = set(word_counts['neg'].keys())\n",
        "    V_bigrams = len(pos_bigrams.union(neg_bigrams))\n",
        "\n",
        "    # 3. Compute total bigram count per class\n",
        "    total_counts = {c: sum(word_counts[c].values()) for c in ['pos', 'neg']}\n",
        "\n",
        "    return word_counts, total_counts, V_bigrams\n",
        "\n",
        "# --- Define Bigram Probability and Classification Functions ---\n",
        "def P_bigram_given_class(bigram: Tuple[str, str], c: str, wc: Dict[str, Counter], tc: Dict[str, int], vocab_size: int) -> float:\n",
        "    \"\"\"Calculates P(bigram | class) using Add-1 smoothing.\"\"\"\n",
        "    return (wc[c].get(bigram, 0) + 1) / (tc[c] + vocab_size)\n",
        "\n",
        "def P_class_given_doc_bigrams(words: List[str], wc: Dict[str, Counter], tc: Dict[str, int], vocab_size: int) -> str:\n",
        "    \"\"\"Classifies a document (list of words) using bigram features.\"\"\"\n",
        "    probs = {}\n",
        "\n",
        "    # Extract bigrams from the test document\n",
        "    lower_words = [w.lower() for w in words]\n",
        "    doc_bigrams = list(ngrams(lower_words, 2))\n",
        "\n",
        "    for c in ['pos', 'neg']:\n",
        "        log_prob = math.log(0.5) # Assume equal priors\n",
        "\n",
        "        for bg in doc_bigrams:\n",
        "            log_prob += math.log(P_bigram_given_class(bg, c, wc, tc, vocab_size))\n",
        "\n",
        "        probs[c] = log_prob\n",
        "\n",
        "    return max(probs, key=probs.get)\n",
        "\n",
        "# --- Evaluate the Bigram Model ---\n",
        "def evaluate_bigrams(test_data: List[Tuple[List[str], str]], wc: Dict[str, Counter], tc: Dict[str, int], vocab_size: int) -> float:\n",
        "    \"\"\"Evaluates the bigram classifier on the test data.\"\"\"\n",
        "    correct = 0\n",
        "    subset_test = test_data[:200]\n",
        "\n",
        "    for words, label in subset_test:\n",
        "        pred = P_class_given_doc_bigrams(words, wc, tc, vocab_size)\n",
        "        if pred == label:\n",
        "            correct += 1\n",
        "\n",
        "    accuracy = correct / len(subset_test)\n",
        "    return accuracy\n",
        "\n",
        "# --- Run the Bigram Naive Bayes Experiment ---\n",
        "print(\"Running Naive Bayes with Bigram Features...\")\n",
        "\n",
        "# 1. Train the model using bigrams (train_raw is from your notebook)\n",
        "wc_bi, tc_bi, V_bi = get_model_params_bigrams(train_raw)\n",
        "\n",
        "# 2. Evaluate the model (test_raw is from your notebook)\n",
        "accuracy_bigram = evaluate_bigrams(test_raw, wc_bi, tc_bi, V_bi)\n",
        "\n",
        "print(f\"\\n--- Exercise 3 (Part 2) Result ---\")\n",
        "print(f\"Baseline (Unigram) Accuracy: {baseline_accuracy:.4f}\")\n",
        "print(f\"Accuracy with Bigram Features: {accuracy_bigram:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiNgCS3Oso0H",
        "outputId": "851b18c9-d991-42bf-ec22-c6b1808cef75"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Naive Bayes with Bigram Features...\n",
            "\n",
            "--- Exercise 3 (Part 2) Result ---\n",
            "Baseline (Unigram) Accuracy: 0.8700\n",
            "Accuracy with Bigram Features: 0.8650\n"
          ]
        }
      ]
    }
  ]
}